# Предварительная обработка и нормализация

![gallery](pics/1.png)

---

## Деление исходной выборки на обучающую и тестовую

* Если выборка малого объема, как правило, для обучения и тестирования используются все 100% исходной
  выборки.
* Если выборка среднего объема(100-500 примеров), она делится на обучающую и тестовую в соотношении 9 :
  1.
* Если выборка большого объема(>500) соотношение 8 : 2.

---

## Тестирование нейронной сети, обучаемой на малой выборке

* Определение структуры нейронной сети.
* Настройка параметров нейронной сети и выбор алгоритма(определение гиперпараметров).
    - Выбор активационных функций и их параметров.
* Обучение нейронной сети.
* Контрастирование нейронной сети.
![gallery](pics/2.png)

* Тестирование нейронной сети.
* Практическое использование нейронной сети.
* Дообучение нейронной сети.

---

## Оценка качества нейросетевых моделей

* Оценка по ошибкам обучения и тестирования.
![gallery](pics/3.png)

![gallery](pics/4.png)

---

## Однослойные перцептроны

**Перцептрон** - нейр. сеть прямого распространения, использующая в качестве активационной функции
сигмоидную логистическую функцию с насыщением.

![gallery](pics/5.png)
![gallery](pics/6.png)
![gallery](pics/7.png)

---

## Условие окончания

1. Достижение предельного количества эпох обучения.
2. Достижение достаточного условия ошибки обучающей или тестовой выборки.
3. Исчерпание предельного физического времени обучения.

![gallery](pics/8.png)

---

## Пример

![gallery](pics/9.png)
![gallery](pics/10.png)

---

## Многослойные перцептроны

Многослойный перцептрон содержит хотя бы 1 скрытый слой.![gallery](pics/11.png)

### Алгоритм обратного распространения ошибки

![gallery](pics/12.png)
![gallery](pics/13.png)

---

# Информация из методы

## Способы нормализации переменных

Наиболее распространенный способ нормализации входных и выходных переменных – **линейная нормализация**.

Примем следующие обозначения:

![gallery](pics/metoda1.png)

![gallery](pics/metoda2.png)

Если обучающая выборка не содержит примеров с потенциально возможными меньшими или большими выходными значениями, можно задаться шириной коридора экстраполяции ψ для левой, правой или обеих границ в долях от длины всего первоначального интервала изменения переменной, обычно не более 10 % от нее. В этом случае происходит переход от фактических границ из обучающей выборки к гипотетическим:

![gallery](pics/metoda3.png)

Один из способов **нелинейной нормализации** – с использованием сигмоидной логистической функции или гиперболического тангенса. Переход от традиционных единиц измерения к нормализованным и обратно в данном случае осуществляется следующим образом:

- при нормализации и денормализации в пределах [0, 1]:

![gallery](pics/metoda4.png)

где xc_i, yc_j – центры нормализуемых интервалов изменения входной и выходной переменных:

![gallery](pics/metoda5.png)

- при нормализации и денормализации в пределах [–1, 1]:

![gallery](pics/metoda6.png)

Параметр α влияет на степень нелинейности изменения переменной в нормализуемом интервале. Кроме того, при использовании значений α < 0,5 нет необходимости дополнительно задаваться шириной коридора экстраполяции.

Рассмотрим в сравнении методы линейной и нелинейной нормализации.На рис. 3.1 приведены графики нормализации входной переменной для пределов [–1; 1]. Для нелинейной нормализации с использованием функции гиперболического тангенса принято значение параметра α = 1,0. Следует отметить, что совпадение нормализованного значения в обоих случаях имеет место лишь в точке, соответствующей центру нормализуемого интервала.

![gallery](pics/metoda7.png)

На рис. 3.2 показаны случаи нелинейной нормализации в пределах [0; 1] с использованием функции гиперболического тангенса с параметрами α, равными, соответственно, 0,3, 0,5, 1,0. Очевидно, что чем меньше значение параметра a, тем более полого выглядит нормализованная зависимость и больше ширина коридора экстраполяции

![gallery](pics/metoda8.png)

---

## Однослойные перцептроны

В современном понимании **перцептроны** представляют собой однослойные или многослойные искусственные нейронные сети прямого распространения с бинарными или аналоговыми выходными сигналами, обучающиеся с учителем. Они хорошо подходят для решения нескольких типов задач: аппроксимации данных, прогнозирования состояния на основе временного ряда, распознавания образов и классификации, а также могут быть использованы в других задачах сами по себе или совместно с другими методами моделирования.

На рис. 8.1 представлена структура однослойного перцептрона с M входами и K выходами. Очевидно, что каждый выход соответствует своему нейрону единственного слоя. Кроме того, ясно, что сложность структуры однослойной сети не может варьироваться ввиду отсутствия скрытых слоев нейронов.

![gallery](pics/metoda9.png)

Количество весовых коэффициентов Nw, настраиваемых в процессе обучения, рассчитывается следующим образом:

![gallery](pics/metoda10.png)

---

Однослойные перцептроны обучаются на основе итерационного **метода Уидроу–Хоффа**, иначе называемого дельта-правилом. Алгоритм данного метода следующий:

1. Весовые коэффициенты однослойного перцептрона выбранной структуры инициализируются небольшими по абсолютной величине (не более M^–1) случайными значениями.

2. На входы перцептрона подается входной вектор одного из примеров обучающей выборки. Производится прямое распространение сигналов по сети с расчетом значений выходных переменных ![gallery](pics/metoda11.png)

3. Для каждого рассчитанного значения выходной переменной вычисляется погрешность по сравнению со значениями элементов выходного вектора взятого обучающего примера ![gallery](pics/metoda12.png)

![gallery](pics/metoda13.png)

4. Выполняется коррекция старых значений весовых коэффициентов каждого нейрона wij^q на основе погрешности соответствующей выходной переменной:

![gallery](pics/metoda14.png)

где v – коэффициент скорости обучения.

5. Цикл повторяется с шага 2 до выполнения одного или нескольких условий окончания:

    - исчерпано заданное предельное количество эпох обучения;
    - достигнут удовлетворительный уровень ошибки по всей обучающей выборке;
    - не происходит уменьшения ошибки обучающей выборки на протяжении заданного предельного количества эпох обучения;
    – исчерпано заданное предельное физическое время обучения

---

Коэффициент скорости обучения задается положительной константой или переменной величиной (0 < v <= 1), постепенно уменьшающейся в процессе обучения нейронной сети.

Общая ошибка работы нейронной сети с обучающей выборкой оценивается по соотношению:

![gallery](pics/metoda15.png)

Также полезно оценивать ошибку работы нейронной сети в отдельности по каждому ее выходу:

![gallery](pics/metoda16.png)

Порядок величины полученных в последнем случае ошибок сопоставим со средней абсолютной ошибкой нормализованных выходных переменных. Как следствие, с ее помощью можно оценить порядок величины абсолютной ошибки в исходных единицах:

![gallery](pics/metoda17.png)

или относительной ошибки в процентах:

![gallery](pics/metoda18.png)

---

## Многослойные перцептроны

Количество весовых коэффициентов, настраиваемых в процессе обучения многослойного перцептрона с L скрытыми слоями по ml нейронов в каждом, рассчитывается следующим образом:

![gallery](pics/metoda19.png)

Для каждого нейрона сети помимо синаптических связей с элементами входного вектора настраивается связь с фиктивным единичным входом(коэффициент смещения).

Для большинства задач, решаемых с помощью многослойных перцептронов, выбор структуры нейронной сети должен осуществляться на основе следующего правила («Правила 2–5»):

***количество настраиваемых в процессе обучения весовых коэффициентов должно быть в 2–5 раз меньше, чем количество примеров обучающей выборки.***

Если это соотношение меньше 2, сеть теряет способность к обобщению обучающей информации, а при достижении 1 и меньше просто запоминает ответы для каждого обучающего примера. Если же количество обучающих примеров слишком велико для выбранной структуры сети, нейросетевая модель во многих случаях просто усредняет выходные значения для различных комбинаций входных векторов, теряя способность к корректному отклику в отдельных частных случаях и повышая величину максимальной выборочной ошибки.
Кроме того, при выборе структуры многослойного перцептрона следует задавать количество нейронов в скрытом слое, предшествующем выходному слою, не меньшим, чем количество самих выходов.

Метод Уидроу–Хоффа, рассмотренный в предыдущей главе, не может быть использован для настройки весовых коэффициентов многослойных перцептронов, поскольку он позволяет сделать прямую коррекцию по величине ошибки только для нейронов выходного слоя, тогда как синаптические коэффициенты скрытых нейронов также требуют изменения.

Наиболее распространенный метод обучения многослойного перцептрона – **метод обратного распространения ошибки**. Суть данного метода заключается в том, что сигнал ошибки каждого выходного значения, рассчитанный на текущем такте обучения, распространяется по слоям в обратном направлении (от выходного к первому) с учетом тех же весовых коэффициентов, которые использовались при прямом прохождении входных сигналов по нейронной сети и расчете выходных значений.

---
Алгоритм метода обратного распространения ошибки включает следующие этапы:

![gallery](pics/metoda20.png)
![gallery](pics/metoda21.png)
![gallery](pics/metoda22.png)
---

Одна из особенностей метода обратного распространения ошибки заключается в том, что средние абсолютные значения невязок уменьшаются от последнего слоя нейронов к первому на порядки. Следствием этого становится практически незначительная величина изменения весовых коэффициентов первых скрытых слоев и, соответственно, требуется очень большое количество эпох обучения для значимой коррекции весов. Для устранения данного недостатка в многослойных перцептронах с более чем одним скрытым слоем можно использовать коэффициент скорости обучения, увеличивающийся от последнего скрытого слоя к первому в пределах одной эпохи обучения, в том числе, значения vl > 1.
