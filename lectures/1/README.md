# Основы ИИ и нейронных сетей

## Определения

**ИИ** - способность искусственно организованных систем брать на себя функции, принадлежащие объектам
естественного происхождения(человеку, животным, социальным системам и т.д.) и использовать принципы и механизмы для
решения различных задач.

**Искусственная нейросеть** - математический аппарат, основанный на аналогах с функционированием ЦНС и
предназначенный для широкого круга математических, творческих, лингвистических и др. задач.

**Архитектура нейросети** - сочетание принципов организации и функционирования нейр. сети, отдельных
иск-х нейронов, алгоритмов и расчетн. отношений.

---

## Решаемые задачи

- Нахождение значения функции по заданным аргументам на основе данных реального или вычислительного
  эксперимента.
- Задачи интерполирования и аппроксимации данных.
- Задачи кластеризации.
- Задачи прогнозирования временных рядов.
- Распознавание и ассоциация образов.
- Классификация и идентификация образов.

---

## Основные элементы нейронных сетей

**Искусственный нейрон** - элементарная структурная единица нейронной сети, выполняющая функцию
обработки сигналов, поступающих с сенсоров или других нейронов и представления результата в форме
выходного значения.

![gallery](pics/1.png)

**Структура нейр. сети** - определяется количественным составом входных и выходных переменных, способом
группирования нейронов в сети, способом организации связи между нейронами и слоями.

**Обучение нейронной сети** - процедура точного или итерационного расчета всех весовых коэффициентов.

**Эпоха обучения** - цикл однократного применения(как правило в случайном порядке) всех примеров
обучающей выборки для коррекции весовых коэффициентов.

---

## Виды функций активации

1. **Гистерезис** - монотонное возрастание и убывание, насыщение вблизи границ и max по абсолютной
   величине скорость изменения функции в центре области определения.

![gallery](pics/2.png)

2. **Импульс** - характеризуется наличием максимума(в искл. случае минимума) функции в центре и насыщение
   вблизи границ области определения.

![gallery](pics/3.png)

3. **Порог** - характеризуется мгновенным изменением выходного значения функции при достижении заданного
   порога.

![gallery](pics/4.png)

---

## Виды гистерезиса

1. Сигмоидная логистическая функция с насыщением σ. область допустимых значений с одной стороны
   стремится к 0 с другой к 1.

2. Линейный порог

![gallery](pics/5.png)

3. Гиперболический тангенс

![gallery](pics/6.png)

---

## Определения

**Слой нейронов** - группа нейронов, соединенных связями 1 уровня с нейронами других групп, входами сети
или друг с другом.

**Выходной слой** - слой нейронов, выходы которого являются выходами всей сети и нейросетевой модели.

**Скрытые слои** - все слои, кроме выходного.

![gallery](pics/7.png)

---

## Классификация

1. По способу обучения:

    - обучение с учителем.
    - обучение без учителя.
    - с подкреплением(нейросеть обучается заданной линией поведения во взаимодействиисо внешней средой).

2. По типу алгоритмов:

    - итерационные алгоритмы.
    - точный алгоритм(веса рассчитываются однократно, по правилу и критерию, заложенному в алгоритм).
    - алгоритм с дообучением.

3. По области допустимых значений входных и выходных сигналов:

    - аналоговые сигналы(бесконечное множество в ограниченном пределе).
    - бинарные сигналы.
    - смешанные.

4. По количеству слоев:

    - однослойные.
    - двухслойные.
    - многослойные.

5. По структуре связи:

    - сети прямого распространения(сигнал распространяется от входа к выходу).
    - сети с обратными связями(рекуррентные), сигналы отдельных нейронов или слоев возвращаются из входов
того же или предыдущих слоев.

6. По режиму изменения состояния нейрона:

    - синхронные(в определенный момент времени изменяется состояние всех нейронов сети или слоя).
    - асинхронные.

7. По форме исполнения нейр. сети:

    - аппаратные.
    - программные.

8. По архитектуре сети:

    - однослойные и многослойные перцептроны.
    - нейронная сеть радиально-базисных функций.
    - нейронная сеть Кохонена.
    - н.с. адаптивного резонанса.
    - н.с. автоассоциативной памяти Хопфилда.
    - генеративно-состязательные.
    - сверточные
    - сети-трансформеры.

---

## Этапы жизненного цикла:

1. Постановка заданий и выбор архитектуры.
2. Определение количественного и качественного составов входов и выходов.
3. формирование исходной выборки данных.

## Требования

1. Соответствие выборки структуре нейронной сети.
2. Уникальность примеров.
3. Непротиворечивость выбора.
4. Репрезентативность - наличие широкого и более-менее распределенного спектра входных данных и всех их
   возможных комбинаций.

---

# Данные из методы

## Активационные функции и их свойства

К активационным функциям типа **гистерезиса** относятся:

- понижающегося единичного скачка (рис. 4.1, а):

![gallery](pics/metoda1.png)

- повышающегося единичного скачка (рис. 4.1, б):

![gallery](pics/metoda2.png)

- линейная пороговая (рис. 4.1, в, г):

![gallery](pics/metoda3.png)

- сигмоидная логистическая (рис. 4.1, д):

![gallery](pics/metoda4.png)

- сигмоидная – гиперболический тангенс (рис. 4.1, е):

![gallery](pics/metoda5.png)

![gallery](pics/metoda6.png)


Варьируя параметр насыщения α логистической функции или гиперболического тангенса, можно добиться изменения вида их графика. В предельных значениях, близких по абсолютному значению к 0 или к бесконечности, графики обеих функций приближаются к форме горизонтальной прямой на уровне середины интервала изменения нормированной переменной или мгновенного скачка (таблица).

**Свойства сигмоидных функций**:

![gallery](pics/metoda7.png)

Еще одно очень важное свойство сигмоидных функций – возможность точного расчета значений их производных только на основе численных значений самих функций. Данное свойство выгодно используется во
многих алгоритмах обучения нейронных сетей.
В соответствии с описанным свойством для сигмоидной логистической функции производную в точке s можно рассчитать по соотношению:

![gallery](pics/metoda8.png)

а для функции гиперболического тангенса:

![gallery](pics/metoda9.png)


