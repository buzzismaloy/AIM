import numpy as np

# Исходные веса нейронов
W = np.array([
    [0.584, 0.340, 0.812],
    [0.793, 0.843, 0.479],
    [0.673, 0.157, 0.358],
    [0.886, 0.183, 0.573]
])

# Обучающие примеры (столбцы - примеры, строки - признаки)
X = np.array([
    [0.663, 0.640, 0.775, 0.333, 0.688],
    [0.561, 0.945, 0.509, 0.745, 0.727],
    [0.302, 0.660, 0.417, 0.129, 0.741],
    [0.606, 0.856, 0.636, 0.183, 0.899]
])


# Повторяем обучение, но теперь с выводом промежуточных вычислений
alpha = 0.5  # Коэффициент скорости обучения
intermediate_results = []

for i, x in enumerate(X.T):  # Перебираем примеры
    # Вычисляем евклидовы расстояния от примера до каждого нейрона
    distances = np.linalg.norm(W - x.reshape(-1, 1), axis=0)
    
    # Определяем нейрон-победитель (с минимальным расстоянием)
    winner_idx = np.argmin(distances)

    # Запоминаем промежуточные результаты
    intermediate_results.append({
        "Пример x": x,
        "Расстояния": distances,
        "Победитель": winner_idx + 1,  # Индексация с 1 для удобства
        "Старые веса": W[:, winner_idx].copy(),
    })

    # Обновляем веса нейрона-победителя
    W[:, winner_idx] += alpha * (x - W[:, winner_idx])

    # Запоминаем обновлённые веса
    intermediate_results[-1]["Новые веса"] = W[:, winner_idx].copy()

# Вывод всех промежуточных расчетов
print(intermediate_results)

# Вывод весов после одной эпохи обучения
print(W)
